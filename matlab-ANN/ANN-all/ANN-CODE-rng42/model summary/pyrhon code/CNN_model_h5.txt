import pandas as pd
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score
from sklearn.model_selection import train_test_split

# Load the data
data = pd.read_csv('edm_all.csv')

X = data.iloc[:, 1:4 ].values  # Including the new feature
y = data.iloc[:, 4:9].values  # Output columns 5, 6, 7, 8, 9


# Normalize the data
scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_scaled = scaler_X.fit_transform(X)
y_scaled = scaler_y.fit_transform(y)

# Reshape X to 3D for Conv1D: (samples, timesteps, features)
X_scaled = np.expand_dims(X_scaled, axis=2)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)

# Define the CNN model with tuning
model = Sequential([
    Conv1D(filters=128, kernel_size=2, activation='relu', input_shape=(X_scaled.shape[1], 1)),
    Dropout(0.3),
    Conv1D(filters=128, kernel_size=2, activation='relu'),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(y.shape[1])
])

# Compile the model with a lower learning rate
model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

# Train the model with more epochs
history = model.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.2, verbose=2)

# Evaluate the model
y_pred_scaled = model.predict(X_test)
y_pred = scaler_y.inverse_transform(y_pred_scaled)
y_test = scaler_y.inverse_transform(y_test)

mse = mean_squared_error(y_test, y_pred)
mape = mean_absolute_percentage_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'MSE: {mse}')
print(f'MAPE: {mape}')
print(f'R-squared: {r2}')

# Save the model
model.save('cnn_model_tuned.h5')

